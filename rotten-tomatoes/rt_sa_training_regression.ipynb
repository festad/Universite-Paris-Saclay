{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "go-MjMhpx5te",
        "outputId": "e60637d0-d9bb-4014-e008-0dd5915ce5ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "ROOT = Path('/content/drive/MyDrive/universite-paris-saclay/nlp/rottentomatoes')"
      ],
      "metadata": {
        "id": "OLB5zDSVyeUB"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "XqlOdA_Rykds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.decomposition import PCA\n",
        "from wordcloud import WordCloud\n",
        "from transformers import BertModel, BertTokenizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.utils.data import TensorDataset, Dataset, DataLoader, random_split\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import random\n",
        "import nltk\n",
        "import gensim\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "ce7oCrXQyqxh"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"omw-1.4\")"
      ],
      "metadata": {
        "id": "l9a0gPc2BjJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_df = pd.read_csv(ROOT / \"regression_reviews.csv\")\n",
        "reviews_df.sample(10, random_state=2023)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "BGQdFMN9y2Hp",
        "outputId": "901fe9b3-8a38-4bb4-faa1-ba8c28a95c3e"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      review_score                                     review_content\n",
              "7958           4.0  robbie janney flawless compelling corrective a...\n",
              "7513           3.5  told solid timeless craftsmanship brickie prot...\n",
              "3105           1.5  get drunk solemn resonance turn intriguing ele...\n",
              "1924           0.5  tacky cgidriven actionhorror flick vacuous wri...\n",
              "4589           2.0  homegrown appeal streetdance get lost sequel o...\n",
              "9689           5.0  sorrentino may well made masterpiece great bea...\n",
              "9806           5.0  one delightful surprise lost none charm surpri...\n",
              "2227           1.0  like marshall last ensemble piece valentine da...\n",
              "3679           1.5  nearbrilliance robert rodriguez achieved sin c...\n",
              "6119           3.0  call kind vaguely smart extremely gory gloop w..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-460332d5-0161-4da0-94f4-a9e93a1291c2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review_score</th>\n",
              "      <th>review_content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7958</th>\n",
              "      <td>4.0</td>\n",
              "      <td>robbie janney flawless compelling corrective a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7513</th>\n",
              "      <td>3.5</td>\n",
              "      <td>told solid timeless craftsmanship brickie prot...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3105</th>\n",
              "      <td>1.5</td>\n",
              "      <td>get drunk solemn resonance turn intriguing ele...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1924</th>\n",
              "      <td>0.5</td>\n",
              "      <td>tacky cgidriven actionhorror flick vacuous wri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4589</th>\n",
              "      <td>2.0</td>\n",
              "      <td>homegrown appeal streetdance get lost sequel o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9689</th>\n",
              "      <td>5.0</td>\n",
              "      <td>sorrentino may well made masterpiece great bea...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9806</th>\n",
              "      <td>5.0</td>\n",
              "      <td>one delightful surprise lost none charm surpri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2227</th>\n",
              "      <td>1.0</td>\n",
              "      <td>like marshall last ensemble piece valentine da...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3679</th>\n",
              "      <td>1.5</td>\n",
              "      <td>nearbrilliance robert rodriguez achieved sin c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6119</th>\n",
              "      <td>3.0</td>\n",
              "      <td>call kind vaguely smart extremely gory gloop w...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-460332d5-0161-4da0-94f4-a9e93a1291c2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-460332d5-0161-4da0-94f4-a9e93a1291c2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-460332d5-0161-4da0-94f4-a9e93a1291c2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First we need to rescale the values from [0,5] to [0,1]\n",
        "reviews_df['review_score'] = reviews_df['review_score'] / 5.0\n",
        "reviews_df.sample(10, random_state=2023)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "cJgwmmWJ1Zx-",
        "outputId": "bbd1f824-d853-45fa-8416-ea759df54397"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      review_score                                     review_content\n",
              "7958           0.8  robbie janney flawless compelling corrective a...\n",
              "7513           0.7  told solid timeless craftsmanship brickie prot...\n",
              "3105           0.3  get drunk solemn resonance turn intriguing ele...\n",
              "1924           0.1  tacky cgidriven actionhorror flick vacuous wri...\n",
              "4589           0.4  homegrown appeal streetdance get lost sequel o...\n",
              "9689           1.0  sorrentino may well made masterpiece great bea...\n",
              "9806           1.0  one delightful surprise lost none charm surpri...\n",
              "2227           0.2  like marshall last ensemble piece valentine da...\n",
              "3679           0.3  nearbrilliance robert rodriguez achieved sin c...\n",
              "6119           0.6  call kind vaguely smart extremely gory gloop w..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4f939991-b409-4781-a218-6d8b66f84ba8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review_score</th>\n",
              "      <th>review_content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7958</th>\n",
              "      <td>0.8</td>\n",
              "      <td>robbie janney flawless compelling corrective a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7513</th>\n",
              "      <td>0.7</td>\n",
              "      <td>told solid timeless craftsmanship brickie prot...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3105</th>\n",
              "      <td>0.3</td>\n",
              "      <td>get drunk solemn resonance turn intriguing ele...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1924</th>\n",
              "      <td>0.1</td>\n",
              "      <td>tacky cgidriven actionhorror flick vacuous wri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4589</th>\n",
              "      <td>0.4</td>\n",
              "      <td>homegrown appeal streetdance get lost sequel o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9689</th>\n",
              "      <td>1.0</td>\n",
              "      <td>sorrentino may well made masterpiece great bea...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9806</th>\n",
              "      <td>1.0</td>\n",
              "      <td>one delightful surprise lost none charm surpri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2227</th>\n",
              "      <td>0.2</td>\n",
              "      <td>like marshall last ensemble piece valentine da...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3679</th>\n",
              "      <td>0.3</td>\n",
              "      <td>nearbrilliance robert rodriguez achieved sin c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6119</th>\n",
              "      <td>0.6</td>\n",
              "      <td>call kind vaguely smart extremely gory gloop w...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4f939991-b409-4781-a218-6d8b66f84ba8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4f939991-b409-4781-a218-6d8b66f84ba8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4f939991-b409-4781-a218-6d8b66f84ba8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased', return_dict=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zuY1oYB3xY6",
        "outputId": "9c4d2485-9fa2-45fe-9a5b-5034238d657f"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function to tokenize a set of texts\n",
        "def preprocessing_for_bert(data, max_len):\n",
        "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
        "    @param    data (np.array): Array of texts to be processed.\n",
        "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
        "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
        "                  tokens should be attended to by the model.\n",
        "    \"\"\"\n",
        "    # Create empty lists to store outputs\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    # For every sentence...\n",
        "    for sent in data:\n",
        "        # `encode_plus` will:\n",
        "        #    (1) Tokenize the sentence\n",
        "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
        "        #    (3) Truncate/Pad sentence to max length\n",
        "        #    (4) Map tokens to their IDs\n",
        "        #    (5) Create attention mask\n",
        "        #    (6) Return a dictionary of outputs\n",
        "        encoded_sent = tokenizer.encode_plus(\n",
        "            text = sent,  # Preprocess sentence\n",
        "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
        "            max_length=max_len,                  # Max length to truncate/pad\n",
        "            pad_to_max_length=True,         # Pad sentence to max length\n",
        "            #return_tensors='pt',           # Return PyTorch tensor\n",
        "            return_attention_mask=True      # Return attention mask\n",
        "            )\n",
        "        \n",
        "        # Add the outputs to the lists\n",
        "        input_ids.append(encoded_sent.get('input_ids'))\n",
        "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
        "\n",
        "    # Convert lists to tensors\n",
        "    input_ids = torch.tensor(input_ids)\n",
        "    attention_masks = torch.tensor(attention_masks)\n",
        "\n",
        "    return input_ids, attention_masks"
      ],
      "metadata": {
        "id": "m4surIir2jj4"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample just the 10% of the data\n",
        "perc = 0.3\n",
        "reviews_df = reviews_df.sample(frac=perc, random_state=2023)\n",
        "\n",
        "X = reviews_df['review_content'].values\n",
        "y = reviews_df['review_score'].values\n",
        "\n",
        "X_train, X_test, y_train, y_test =\\\n",
        "    train_test_split(X, y, test_size=0.1, random_state=2023)\n",
        "\n",
        "X_train, X_val, y_train, y_val =\\\n",
        "    train_test_split(X_train, y_train, test_size=0.1, random_state=2024)\n",
        "\n",
        "# Tokenize the input text\n",
        "\n",
        "max_len = 30\n",
        "\n",
        "# Run function `preprocessing_for_bert` on the train set, the validation set and the test set\n",
        "train_inputs, train_masks = preprocessing_for_bert(X_train, max_len)\n",
        "val_inputs, val_masks = preprocessing_for_bert(X_val, max_len)\n",
        "test_inputs, test_masks = preprocessing_for_bert(X_test, max_len)\n",
        "\n",
        "# Convert other data types to torch.Tensor\n",
        "train_labels = torch.tensor(y_train)\n",
        "val_labels = torch.tensor(y_val)\n",
        "test_labels = torch.tensor(y_test)\n",
        "\n",
        "# For fine-tuning BERT, it's recommended a batch size of 16 or 32.\n",
        "batch_size = 16\n",
        "\n",
        "# Create the DataLoader for our training set\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set\n",
        "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our test set\n",
        "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=1)"
      ],
      "metadata": {
        "id": "cNVo4weM3QCS"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():       \n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
        "    print('Device name:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mHKXSzb4Cec",
        "outputId": "71b34040-f9a8-48b0-a6e3-1cd045cd3acf"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "Device name: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BertForRegression(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BertForRegression, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.linear = nn.Linear(self.bert.config.hidden_size, 1)\n",
        "        self.double()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs[1]  # Use the second output (pooled output)\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        regression_output = self.linear(pooled_output)\n",
        "        return regression_output.squeeze(-1)"
      ],
      "metadata": {
        "id": "eJMnd9rA4LCT"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_dataloader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for i, batch in enumerate(train_dataloader):\n",
        "        input_ids, attention_mask, labels = tuple(t.to(device) for t in batch)\n",
        "        # input_ids = batch['input_ids'].to(device)\n",
        "        # attention_mask = batch['attention_mask'].to(device)\n",
        "        # labels = batch['label'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(train_dataloader)\n",
        "\n",
        "\n",
        "def evaluate(model, val_dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(val_dataloader):\n",
        "            input_ids, attention_mask, labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(val_dataloader)\n"
      ],
      "metadata": {
        "id": "W3rUNp_H4TYh"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertForRegression()\n",
        "model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "train_losses = []\n",
        "# train_accs = []\n",
        "val_losses = []\n",
        "# val_accs = []\n",
        "\n",
        "num_epochs = 5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train(model, train_dataloader, optimizer, criterion, device)\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(train_loss)\n",
        "    val_loss = evaluate(model, val_dataloader, criterion, device)\n",
        "    print(f\"Epoch {epoch+1}. Train loss: {train_loss:.5f}. Validation loss: {val_loss:.5f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1oHwiLK5cyL",
        "outputId": "f8344776-ce7e-4723-afa3-0cb82c20c593"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1. Train loss: 0.13284. Validation loss: 0.10035\n",
            "Epoch 2. Train loss: 0.09490. Validation loss: 0.07012\n",
            "Epoch 3. Train loss: 0.05790. Validation loss: 0.07721\n",
            "Epoch 4. Train loss: 0.04728. Validation loss: 0.06238\n",
            "Epoch 5. Train loss: 0.02902. Validation loss: 0.06602\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), ROOT / \"bert-rottentomatoes-regression-03-v2.pth\")"
      ],
      "metadata": {
        "id": "X9zksqof9D48"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BertForRegression(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BertForRegression, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.linear = nn.Linear(self.bert.config.hidden_size, 1)\n",
        "        self.double()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs[1]  # Use the second output (pooled output)\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        regression_output = self.linear(pooled_output)\n",
        "        return regression_output.squeeze(-1)"
      ],
      "metadata": {
        "id": "KcuSHHRL9LCw"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertForRegression()\n",
        "model.load_state_dict(torch.load(ROOT / \"bert-rottentomatoes-regression-01-v1.pth\"))\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "w4-iMpqo9Uwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, test_dataloader):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "        for batch in test_dataloader:\n",
        "            input_ids, attention_mask, _ = tuple(t.to(device) for t in batch)\n",
        "            # input_ids = batch['input_ids'].to(device)\n",
        "            # attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            predictions.extend(outputs.tolist())\n",
        "\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "qeKFtMTw965N"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = predict(model, test_dataloader)\n",
        "targets = y_test\n",
        "\n",
        "mse = mean_squared_error(targets, predictions)\n",
        "rmse = mean_squared_error(targets, predictions, squared=False)\n",
        "mae = mean_absolute_error(targets, predictions)\n",
        "r2 = r2_score(targets, predictions)\n",
        "\n",
        "print(f\"MSE: {mse:.5f}\")\n",
        "print(f\"RMSE: {rmse:.5f}\")\n",
        "print(f\"MAE: {mae:.5f}\")\n",
        "print(f\"R2: {r2:.5f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hD2_UqCl-Pu8",
        "outputId": "3e312b0b-ad38-4980-a8b6-2ec45720e1ed"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 0.11688\n",
            "RMSE: 0.34187\n",
            "MAE: 0.29769\n",
            "R2: -0.04755\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_test(model, text):\n",
        "    input = tokenizer(text, return_tensors='pt')\n",
        "\n",
        "    # Make a prediction with the model\n",
        "    output = model(input['input_ids'], input['attention_mask'])\n",
        "    return output.item() * 5.0"
      ],
      "metadata": {
        "id": "NNF38IRQAMw1"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    # Removing numerical data or irrelevant characters\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
        "    # Tokenization\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    # Lowercasing\n",
        "    tokens = [token.lower() for token in tokens]\n",
        "    # Removing stopwords\n",
        "    tokens = [token for token in tokens if token not in stopwords.words(\"english\")]\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    # Joining the tokens back to a string\n",
        "    text = \" \".join(tokens)\n",
        "    return text"
      ],
      "metadata": {
        "id": "wBJTJnGuBBv2"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BertForRegression()\n",
        "model.load_state_dict(torch.load(ROOT / \"bert-rottentomatoes-regression-01-v1.pth\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdWyCi94A4w0",
        "outputId": "2bb30671-06a9-43c5-f6e3-c7aca029a1ec"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reviews = pd.read_csv(ROOT / \"clean_reviews.csv\")\n",
        "reviews = reviews.sample(20, random_state=2023)\n",
        "\n",
        "for r in reviews.iterrows():\n",
        "    _, r = r\n",
        "    review_type, review_score, review_content = r\n",
        "    # print(r)eview_type, review_score, review_content = r\n",
        "    print('REVIEW:')\n",
        "    print(review_content)\n",
        "    print('True score:', review_score)\n",
        "    print('Prediction:', custom_test(model, preprocess_text(review_content)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4LCCYG4A-Sc",
        "outputId": "bb785fee-220f-4895-f24a-49b9cd14106a"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "REVIEW:\n",
            "THE WORST FILM OF THE YEAR.\n",
            "True score: 0.0\n",
            "Prediction: 3.260732027381536\n",
            "REVIEW:\n",
            "Offers creative action, a deep story, gorgeously colorful visuals, and enough characters to grow and love.\n",
            "True score: 3.5\n",
            "Prediction: 1.8287923215176005\n",
            "REVIEW:\n",
            "The film takes some deciphering, but once a viewer cracks its code \"Alps\" opens up into something expansive and rich.\n",
            "True score: 4.0\n",
            "Prediction: 1.9253423299907708\n",
            "REVIEW:\n",
            "While Lola is adorable throughout, neither she nor the film will be remembered the morning after.\n",
            "True score: 3.0\n",
            "Prediction: 2.475978165896014\n",
            "REVIEW:\n",
            "What's left is a sequence of downbeat sex skits, slotted together with all the finesse of an Ikea cabinet.\n",
            "True score: 2.0\n",
            "Prediction: 2.948368115636786\n",
            "REVIEW:\n",
            "Aamir Khan is stupendous as the rule-breaker Rancho. But the rest of the cast doesn't remain in the shadows.\n",
            "True score: 5.0\n",
            "Prediction: 2.165789137607041\n",
            "REVIEW:\n",
            "Paul Walter Hauser is the only reason to see this film. What we are witnessing with Hauser is the emergence of a riveting actor who dominates every scene he's in, not with force, but with sheer, unadulterated talent.\n",
            "True score: 2.5\n",
            "Prediction: 2.4864765510525517\n",
            "REVIEW:\n",
            "A tense and terrific thriller. Tyler is a standout.\n",
            "True score: 3.0\n",
            "Prediction: 2.695226607291619\n",
            "REVIEW:\n",
            "You'll leave the film knowing more than you might have imagined about Irish culture, but never once feeling subjected to anything Very Special or Good for You.\n",
            "True score: 4.5\n",
            "Prediction: 3.09885827860923\n",
            "REVIEW:\n",
            "There's plenty of action, but little to get excited about.\n",
            "True score: 1.0\n",
            "Prediction: 2.38776744670932\n",
            "REVIEW:\n",
            "Although the vrit aesthetics of Monsters will invite comparisons to Cloverfield and District 9, what galls most is the infuriating lack of purpose beyond its backdrop.\n",
            "True score: 2.5\n",
            "Prediction: 2.102469100151728\n",
            "REVIEW:\n",
            "Few films recently have affected me as powerfully as Lore. It's an outstanding achievement from one of our most talented filmmakers.\n",
            "True score: 4.5\n",
            "Prediction: 3.1469230883152792\n",
            "REVIEW:\n",
            "The emotional stakes never really feel all that high throughout.\n",
            "True score: 3.5\n",
            "Prediction: 2.710523070865362\n",
            "REVIEW:\n",
            "It's the gentle, believable father-daughter chemistry between Murphy and Shahidi that emerges as the film's most impressive effect; it's enough.\n",
            "True score: 3.0\n",
            "Prediction: 2.0577172411168276\n",
            "REVIEW:\n",
            "Baumbach narrates the miseries and glories of a breakup with extreme sensitivity and epic breath. [Full Review in Spanish]\n",
            "True score: 5.0\n",
            "Prediction: 3.578606456327062\n",
            "REVIEW:\n",
            "Although unpredictable, explosive and engrossing, Hold the Dark doesn't find the depth and resonance it seeks.\n",
            "True score: 3.0\n",
            "Prediction: 3.0683946675518374\n",
            "REVIEW:\n",
            "Marci X is the kind of terrible movie I'm almost glad I've seen, because this decade probably won't bring a worse movie than this.\n",
            "True score: 0.0\n",
            "Prediction: 2.4974774650879814\n",
            "REVIEW:\n",
            "If this Nobel prize laureate is not a major world leader by the time she's 30, then there is something irreparably wrong with the world.\n",
            "True score: 4.0\n",
            "Prediction: 3.346230620412574\n",
            "REVIEW:\n",
            "Is there anything less relatable than rich, successful white people whining about how hard they have it?\n",
            "True score: 0.5\n",
            "Prediction: 2.1779421021148773\n",
            "REVIEW:\n",
            "It's no wonder the faithful continue to forsake the movies, given junky embarrassments like Nights in Rodanthe.\n",
            "True score: 0.5\n",
            "Prediction: 2.634400175791617\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================== #\n",
        "# =============== Manual Testing =============== #\n",
        "# ============================================== #\n"
      ],
      "metadata": {
        "id": "Ci5c2Dv5DE5G"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CLASSIFICATION\n",
        "\n",
        "class BertForSentimentAnalysis(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BertForSentimentAnalysis, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.classifier = nn.Linear(768, 2) # 768 is the output dimension of BERT model\n",
        "    \n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "wXpFTSNJDbvu"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# REGRESSION\n",
        "\n",
        "class BertForRegression(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BertForRegression, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.linear = nn.Linear(self.bert.config.hidden_size, 1)\n",
        "        self.double()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs[1]  # Use the second output (pooled output)\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        regression_output = self.linear(pooled_output)\n",
        "        return regression_output.squeeze(-1)"
      ],
      "metadata": {
        "id": "tws2j5NDDhk0"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_classification = BertForSentimentAnalysis()\n",
        "model_classification.load_state_dict(torch.load(ROOT / \"bert-rottentomatoes-binary-100-v4.pth\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUKrbhVjDnpg",
        "outputId": "7de17402-9332-45fd-f491-fe1629b2ffa9"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_regression = BertForRegression()\n",
        "model_regression.load_state_dict(torch.load(ROOT / \"bert-rottentomatoes-regression-03-v2.pth\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVtythj2DVwR",
        "outputId": "17980f81-f225-4418-a810-787aa7a2ab8f"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "gcjyKplmEdVf"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    # Removing numerical data or irrelevant characters\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
        "    # Tokenization\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    # Lowercasing\n",
        "    tokens = [token.lower() for token in tokens]\n",
        "    # Removing stopwords\n",
        "    tokens = [token for token in tokens if token not in stopwords.words(\"english\")]\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    # Joining the tokens back to a string\n",
        "    text = \" \".join(tokens)\n",
        "    return text"
      ],
      "metadata": {
        "id": "DH1Hyqz6EgPM"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_classification_test(model, text):\n",
        "    inputs = tokenizer(text, return_tensors='pt')\n",
        "\n",
        "    # Make a prediction with the model\n",
        "    outputs = model(inputs['input_ids'], inputs['attention_mask'])\n",
        "    predicted_class = torch.argmax(outputs[0])\n",
        "\n",
        "    # Print the predicted sentiment\n",
        "    return 'Fresh' if predicted_class == 1 else 'Rotten'"
      ],
      "metadata": {
        "id": "IPKTm5_fGXEi"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_regression_test(model, text):\n",
        "    input = tokenizer(text, return_tensors='pt')\n",
        "    output = model(input['input_ids'], input['attention_mask'])\n",
        "    return output.item() * 5.0"
      ],
      "metadata": {
        "id": "vTWVL_6HE136"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews = pd.read_csv(ROOT / \"clean_reviews.csv\")\n",
        "reviews = reviews.sample(50, random_state=2025)\n",
        "\n",
        "for r in reviews.iterrows():\n",
        "    _, r = r\n",
        "    review_type, review_score, review_content = r\n",
        "    # print(r)eview_type, review_score, review_content = r\n",
        "    print('REVIEW:')\n",
        "    print(review_content)\n",
        "\n",
        "    text = preprocess_text(review_content)\n",
        "\n",
        "    print('True type:', 'Rotten' if review_type == 0 else 'Fresh')\n",
        "    print('Predicted type:', custom_classification_test(model_classification, text))\n",
        "\n",
        "    print('True score:', review_score)\n",
        "    print('Predicted score:', custom_regression_test(model_regression, text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7J__gIJdEj6g",
        "outputId": "eed0eefe-f398-4ea6-ff17-c84ee51bbeaa"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "REVIEW:\n",
            "A pedal-to-the-metal action adventure - with very little under the hood.\n",
            "True type: Rotten\n",
            "Predicted type: Rotten\n",
            "True score: 2.5\n",
            "Predicted score: 3.579523364042622\n",
            "REVIEW:\n",
            "Any resemblence to Clive Barker's real work is purely coincidental.\n",
            "True type: Rotten\n",
            "Predicted type: Rotten\n",
            "True score: 0.0\n",
            "Predicted score: 2.742251743557361\n",
            "REVIEW:\n",
            "Two reasons not to go see Abduction: first, every screening is guaranteed to be jam-packed with rabid girls squealing every time its star, Twilight's Taylor Lautner is on screen. Second: it's just not a good movie.\n",
            "True type: Rotten\n",
            "Predicted type: Rotten\n",
            "True score: 1.0\n",
            "Predicted score: 1.1469731499778335\n",
            "REVIEW:\n",
            "It underlines the grand feats the human body is capable of and how far it can be stretched through a crazy, nerve-shredding expedition.\n",
            "True type: Fresh\n",
            "Predicted type: Fresh\n",
            "True score: 3.5\n",
            "Predicted score: 4.110241019940163\n",
            "REVIEW:\n",
            "As misguided a film as you're likely to see from a genuinely great filmmaker.\n",
            "True type: Rotten\n",
            "Predicted type: Rotten\n",
            "True score: 1.5\n",
            "Predicted score: 0.9744732722654832\n",
            "REVIEW:\n",
            "Hollywood's slick-looking but dumbed-down remake, directed by first-time feature filmmaker and commercial veteran Jim Sonzero, hits a new low.\n",
            "True type: Rotten\n",
            "Predicted type: Rotten\n",
            "True score: 0.5\n",
            "Predicted score: 1.647152095386202\n",
            "REVIEW:\n",
            "Within 20 minutes of this incongruous baloney, I knew I'd stumbled upon a potential Razzie.\n",
            "True type: Rotten\n",
            "Predicted type: Rotten\n",
            "True score: 0.5\n",
            "Predicted score: -0.1098730504530594\n",
            "REVIEW:\n",
            "Most of the movie is spent watching Jack Gyllenhaal shiver ominously next to Steve Urkel.\n",
            "True type: Rotten\n",
            "Predicted type: Rotten\n",
            "True score: 2.0\n",
            "Predicted score: 2.0445429422646035\n",
            "REVIEW:\n",
            "If you watched the news while all this was going down, you don't really need to see this movie, but it's good that it exists.\n",
            "True type: Fresh\n",
            "Predicted type: Fresh\n",
            "True score: 3.0\n",
            "Predicted score: 2.3854517416335583\n",
            "REVIEW:\n",
            "Guns, money, drink in violent, profane Guy Ritchie caper.\n",
            "True type: Fresh\n",
            "Predicted type: Fresh\n",
            "True score: 4.0\n",
            "Predicted score: 0.6698480412505025\n",
            "REVIEW:\n",
            "Paranormal Activity taps into our primal fears of the unknown to provide a procession of increasingly nervewracking and frenzied moments of terror. Cut off any distractions, glue your eyes and ears to the screen and prepare to be scared silly.\n",
            "True type: Fresh\n",
            "Predicted type: Fresh\n",
            "True score: 5.0\n",
            "Predicted score: 2.850979801650993\n",
            "REVIEW:\n",
            "You've really got to wonder sometimes how talented people end up in hopeless misfires like this film.\n",
            "True type: Rotten\n",
            "Predicted type: Rotten\n",
            "True score: 1.5\n",
            "Predicted score: 3.9219721247783\n",
            "REVIEW:\n",
            "This is easily the best thing Everett has done - and if anything comes from the tragedy of the film, it's welcoming him back into the fray - cinema is a better place with Rupert Everett giving it what for.\n",
            "True type: Fresh\n",
            "Predicted type: Fresh\n",
            "True score: 4.0\n",
            "Predicted score: 3.3234697311023083\n",
            "REVIEW:\n",
            "It is bad in practically every way it could be bad, from concept to execution, from imagery to acting. It is a movie with essentially no redeeming qualities -- the epitome of everything wrong with big-budget moviemaking today.\n",
            "True type: Rotten\n",
            "Predicted type: Rotten\n",
            "True score: 0.5\n",
            "Predicted score: 0.5676720820679656\n",
            "REVIEW:\n",
            "This one is just too obvious, almost from the title.\n",
            "True type: Rotten\n",
            "Predicted type: Rotten\n",
            "True score: 1.0\n",
            "Predicted score: 3.28026343831497\n",
            "REVIEW:\n",
            "'Super 8' can be simplified as 'E.T.' meets 'Cloverfield'... [it] truly is the beginning and the end of summer blockbusters...\n",
            "True type: Fresh\n",
            "Predicted type: Fresh\n",
            "True score: 4.5\n",
            "Predicted score: 4.521429061291001\n",
            "REVIEW:\n",
            "A hit and miss comedy that's never terrible and never great.\n",
            "True type: Rotten\n",
            "Predicted type: Fresh\n",
            "True score: 2.5\n",
            "Predicted score: 2.9041380189211923\n",
            "REVIEW:\n",
            "Ultimately, the film is too neat, and too cloyingly sweet to tackle anything resembling real life.\n",
            "True type: Fresh\n",
            "Predicted type: Fresh\n",
            "True score: 3.0\n",
            "Predicted score: 3.0674583357255134\n",
            "REVIEW:\n",
            "A remarkably engaging, observant approach to a prickly situation\n",
            "True type: Fresh\n",
            "Predicted type: Fresh\n",
            "True score: 4.5\n",
            "Predicted score: 4.161434348642997\n",
            "REVIEW:\n",
            "One of the year's most refreshing surprise packages and exactly the breath of fresh levity that this cinematic universe desperately needs right now.\n",
            "True type: Fresh\n",
            "Predicted type: Fresh\n",
            "True score: 4.5\n",
            "Predicted score: 4.326042582078106\n",
            "REVIEW:\n",
            "A disjointed and tedious action-packed romancer featuring the miscast leads of Nolte and Roberts who toil along in this dull and draining ditty\n",
            "True type: Rotten\n",
            "Predicted type: Rotten\n",
            "True score: 2.0\n",
            "Predicted score: 1.5659729576489592\n",
            "REVIEW:\n",
            "Memoirs of a Geisha is grand entertainment, telling its epic story in bold yet graceful terms without ever descending to melodrama or soap opera.\n",
            "True type: Fresh\n",
            "Predicted type: Fresh\n",
            "True score: 5.0\n",
            "Predicted score: 4.775690219382823\n",
            "REVIEW:\n",
            "It's a heist movie. That's all it is, a fun heist movie with powers and explosions and awesome costumes and ant armies. It's also extremely well directed.\n",
            "True type: Fresh\n",
            "Predicted type: Fresh\n",
            "True score: 5.0\n",
            "Predicted score: 3.551992734364716\n",
            "REVIEW:\n",
            "[The Punisher's] brooding, even suicidal moments are decently handled. It is most of the rest of the movie that is a disaster.\n",
            "True type: Rotten\n",
            "Predicted type: Rotten\n",
            "True score: 1.5\n",
            "Predicted score: 1.249205163801708\n",
            "REVIEW:\n",
            "A film adaptation with big-buck effects, but no soul to speak of.\n",
            "True type: Rotten\n",
            "Predicted type: Rotten\n",
            "True score: 0.5\n",
            "Predicted score: 4.075301440104519\n",
            "REVIEW:\n",
            "What seems to keep you in your seat through this vacuum is wondering how far an incredible premise can be taken under the guise of Sci-Fi.\n",
            "True type: Rotten\n",
            "Predicted type: Rotten\n",
            "True score: 2.5\n",
            "Predicted score: 3.721297691203075\n",
            "REVIEW:\n",
            "An amazingly uneven and increasingly hysterical horror/melodrama that is so over the top that even the central performance of Jennifer Lawrence can't redeem it.\n",
            "True type: Rotten\n",
            "Predicted type: Rotten\n",
            "True score: 2.5\n",
            "Predicted score: 1.4418339643007358\n",
            "REVIEW:\n",
            "The last 30 seconds of The Dark Knight Rises very nearly had me on my feet cheering. It's just all the stuff in the middle that needs pruning.\n",
            "True type: Fresh\n",
            "Predicted type: Fresh\n",
            "True score: 3.0\n",
            "Predicted score: 0.3634407039791831\n",
            "REVIEW:\n",
            "[Cohen's] insights are as worthless as a lecher's promise and the issues he raises as stale as a couch crumb.\n",
            "True type: Rotten\n",
            "Predicted type: Rotten\n",
            "True score: 0.5\n",
            "Predicted score: -0.026534949603756783\n",
            "REVIEW:\n",
            "It soon has enough surprises and funny moments to keep you watching to the end.\n",
            "True type: Fresh\n",
            "Predicted type: Fresh\n",
            "True score: 3.5\n",
            "Predicted score: 2.826936271294968\n",
            "REVIEW:\n",
            "The cast is worth braving multiplex crowds to see. McAdams earns her comedienne wings, and Bateman's subversive deadpan, which always works, works better than usual.\n",
            "True type: Fresh\n",
            "Predicted type: Fresh\n",
            "True score: 4.0\n",
            "Predicted score: 0.181480439191954\n",
            "REVIEW:\n",
            "An Angel at My Table is, like many of Campion's films, a tale of the indomitability of the human spirit.\n",
            "True type: Fresh\n",
            "Predicted type: Fresh\n",
            "True score: 4.5\n",
            "Predicted score: 5.082096435214233\n",
            "REVIEW:\n",
            "A very conventional melodrama about loss and hope. [Full review in Spanish]\n",
            "True type: Rotten\n",
            "Predicted type: Rotten\n",
            "True score: 2.5\n",
            "Predicted score: 3.147358583709627\n",
            "REVIEW:\n",
            "Fast-moving pulp\n",
            "True type: Fresh\n",
            "Predicted type: Fresh\n",
            "True score: 3.0\n",
            "Predicted score: 2.1923264017605355\n",
            "REVIEW:\n",
            "A hilarious and beguiling comedy-adventure-mystery-romance hybrid. And it's not just the jaw-dropping oddity of the thing that makes it work; the film has a wonderfully involving -- and even moving -- storyline.\n",
            "True type: Fresh\n",
            "Predicted type: Fresh\n",
            "True score: 4.5\n",
            "Predicted score: 3.9695316930216955\n",
            "REVIEW:\n",
            "If you've never seen movie explosions before, you're in for a treat. Otherwise, you've got lots of hard-to-follow, poorly lit, very loud chaos to look forward to.\n",
            "True type: Rotten\n",
            "Predicted type: Rotten\n",
            "True score: 1.5\n",
            "Predicted score: 1.2081418958761971\n",
            "REVIEW:\n",
            "The best thing I can say about Escape From Planet Earth is that I didn't end up hating it as much as I thought I was going to during the first half.\n",
            "True type: Rotten\n",
            "Predicted type: Rotten\n",
            "True score: 0.0\n",
            "Predicted score: 2.3444888986366403\n",
            "REVIEW:\n",
            "Shocking, eerie, doom-laden, and discomforting, it's a horror movie that truly gets under your skin.\n",
            "True type: Fresh\n",
            "Predicted type: Fresh\n",
            "True score: 4.5\n",
            "Predicted score: 1.7470377415421936\n",
            "REVIEW:\n",
            "The sense of period is strong and made especially real in scenes in which royalty find themselves incongruously in small, distinctly non-palatial homes.\n",
            "True type: Fresh\n",
            "Predicted type: Fresh\n",
            "True score: 4.0\n",
            "Predicted score: 2.5817835324765226\n",
            "REVIEW:\n",
            "This laughable yet laugh-inducing documentary has probably helped save him from the financial ruin that he was trying to escape with his cocaine adventure. Maybe he wasn't so stupid after all.\n",
            "True type: Fresh\n",
            "Predicted type: Fresh\n",
            "True score: 3.0\n",
            "Predicted score: 1.355786792884381\n",
            "REVIEW:\n",
            "What makes this textbook set-up sing is the pitch-perfect casting of Four Weddings and a Funeral star Kristin Scott Thomas and having her charge head on at Catastrophe lead Sharon Horgan as Lisa, her fiery co-choir leader\n",
            "True type: Fresh\n",
            "Predicted type: Fresh\n",
            "True score: 3.5\n",
            "Predicted score: 1.4302463247089505\n",
            "REVIEW:\n",
            "Prom Night is poorly written, ill conceived, and executed with the excitement of a children's cough medicine commercial.\n",
            "True type: Rotten\n",
            "Predicted type: Rotten\n",
            "True score: 0.5\n",
            "Predicted score: 1.8832792186736493\n",
            "REVIEW:\n",
            "Is The Mummy any good? Not really, but it's OK. Is it good enough to launch a franchise? Absolutely not.\n",
            "True type: Rotten\n",
            "Predicted type: Rotten\n",
            "True score: 2.5\n",
            "Predicted score: 4.260792315511116\n",
            "REVIEW:\n",
            "The lugubrious tone, studded with murky references to Edgar Allen Poe and shaded with forbidden sexual longing, may strike some as a little tiresome. But the set design is amazing.\n",
            "True type: Fresh\n",
            "Predicted type: Fresh\n",
            "True score: 3.0\n",
            "Predicted score: 3.6920305644154094\n",
            "REVIEW:\n",
            "It is a tedious marathon of smoke and mirrors. In terms of the basic requirements of three-reel drama the film lacks substance, credibility, a decent script and characters you might actually care for.\n",
            "True type: Rotten\n",
            "Predicted type: Rotten\n",
            "True score: 2.0\n",
            "Predicted score: 1.382258328416818\n",
            "REVIEW:\n",
            "it almost makes you want to go out into the multiplex parking lot afterwards and plaintively call out \"Come back, 'John Carter'--all is forgiven!\"\n",
            "True type: Rotten\n",
            "Predicted type: Rotten\n",
            "True score: 1.0\n",
            "Predicted score: 1.5305352816248499\n",
            "REVIEW:\n",
            "It's a lavish production but an uneven one.\n",
            "True type: Fresh\n",
            "Predicted type: Rotten\n",
            "True score: 3.0\n",
            "Predicted score: 1.1322590604992795\n",
            "REVIEW:\n",
            "Bardem's character, police detective Agustin Rejas, is Malkovich's attempt to place high minded dedication, intelligence and sensitivity into a system sustained by corruption, greed and unfettered power.\n",
            "True type: Rotten\n",
            "Predicted type: Fresh\n",
            "True score: 2.5\n",
            "Predicted score: 2.7159186476643002\n",
            "REVIEW:\n",
            "The first 30 minutes of the film, where this concept is really milked to the max, easily makes for some of the funniest comedy of the year.\n",
            "True type: Fresh\n",
            "Predicted type: Fresh\n",
            "True score: 4.0\n",
            "Predicted score: 4.694255259184555\n",
            "REVIEW:\n",
            "The movie reaches the heights of melodrama with narrative cliches that border on the surreal.\n",
            "True type: Rotten\n",
            "Predicted type: Rotten\n",
            "True score: 2.0\n",
            "Predicted score: 4.149947032878769\n"
          ]
        }
      ]
    }
  ]
}